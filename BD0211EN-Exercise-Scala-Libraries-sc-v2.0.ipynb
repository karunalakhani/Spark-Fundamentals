{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals I - Introduction to Spark</h1>\n",
    "<h2 align = \"center\"> Scala - Working with Scala Libraries</h2>\n",
    "<br align = \"left\">\n",
    "\n",
    "**Related free online courses:**\n",
    "\n",
    "Related courses can be found in the following learning paths:\n",
    "\n",
    "- [Spark Fundamentals path](http://cocl.us/Spark_Fundamentals_Path)\n",
    "- [Big Data Fundamentals path](http://cocl.us/Big_Data_Fundamentals_Path)\n",
    "\n",
    "<img src = \"http://spark.apache.org/images/spark-logo.png\", height = 100, align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Spark application using Spark SQL\n",
    "\n",
    "Spark SQL provides the ability to write relational queries to be run on Spark. There is the abstraction SchemaRDD which is to create an RDD in which you can run SQL, HiveQL, and Scala. In this lab section, you will use SQL to find out the average weather and precipitation for a given time period in New York. The purpose is to demonstrate how to use the Spark SQL libraries on Spark.\n",
    "\n",
    "### Please note that in Spark 1.3 DataFrames have replaced schemaRDDs however, it is still possible to switch between the two for supporting legacy systems. DataFrames is the recommended method going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first download the data that we will be working with in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// download module to run shell commands within this notebook\n",
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:32: error: type mismatch;\n",
       " found   : Unit\n",
       " required: scala.sys.process.ProcessLogger\n",
       "       println(\"Data Downloaded!\")\n",
       "              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// download data from IBM Servier\n",
    "// this may take ~30 seconds depending on your internet speed\n",
    "\"wget --quiet https://ibm.box.com/shared/static/j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "println(\"Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unzip the data that we just downloaded into a directory dedicated for this course. Let's choose the directory **/resources/jupyter/labs/BD0211EN/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: type mismatch;\n",
       " found   : Unit\n",
       " required: scala.sys.process.ProcessLogger\n",
       "       println(\"Data Extracted!\")\n",
       "              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// unzip the folder's content into \"resources\" directory\n",
    "\"unzip -q -o -d /resources/jupyterlab/labs/BD0211EN/ j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "println(\"Data Extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a folder called **LabData**. Let's list all the files in the data that we just downloaded and extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "followers.txt\n",
      "notebook.log\n",
      "nyctaxi.csv\n",
      "nyctaxi100.csv\n",
      "nyctaxisub.csv\n",
      "nycweather.csv\n",
      "pom.xml\n",
      "taxistreams.py\n",
      "users.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// list the extracted files\n",
    "\"ls -1 /resources/jupyterlab/labs/BD0211EN/LabData/\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the nycweather data. So run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2013-01-01\",1,0\n",
      "\"2013-01-02\",-2,0\n",
      "\"2013-01-03\",-2,0\n",
      "\"2013-01-04\",1,0\n",
      "\"2013-01-05\",3,0\n",
      "\"2013-01-06\",4,0\n",
      "\"2013-01-07\",5,0\n",
      "\"2013-01-08\",6,0\n",
      "\"2013-01-09\",7,0\n",
      "\"2013-01-10\",7,0\n",
      "\"2013-01-11\",6,13.97\n",
      "\"2013-01-12\",7,0.51\n",
      "\"2013-01-13\",8,0\n",
      "\"2013-01-14\",8,2.29\n",
      "\"2013-01-15\",3,3.05\n",
      "\"2013-01-16\",2,17.53\n",
      "\"2013-01-17\",4,0\n",
      "\"2013-01-18\",-1,0\n",
      "\"2013-01-19\",5,0\n",
      "\"2013-01-20\",6,0\n",
      "\"2013-01-21\",-2,0\n",
      "\"2013-01-22\",-7,0\n",
      "\"2013-01-23\",-9,0\n",
      "\"2013-01-24\",-8,0\n",
      "\"2013-01-25\",-7,1.78\n",
      "\"2013-01-26\",-6,0\n",
      "\"2013-01-27\",-3,0\n",
      "\"2013-01-28\",1,5.59\n",
      "\"2013-01-29\",6,1.52\n",
      "\"2013-01-30\",9,1.02\n",
      "\"2013-01-31\",8,22.86\n",
      "\"2013-02-01\",-2,0\n",
      "\"2013-02-02\",-4,0.51\n",
      "\"2013-02-03\",-3,0.51\n",
      "\"2013-02-04\",-3,0\n",
      "\"2013-02-05\",-1,0.51\n",
      "\"2013-02-06\",1,0\n",
      "\"2013-02-07\",-2,0\n",
      "\"2013-02-08\",-1,29.21\n",
      "\"2013-02-09\",-3,9.65\n",
      "\"2013-02-10\",-3,0\n",
      "\"2013-02-11\",4,12.45\n",
      "\"2013-02-12\",4,0\n",
      "\"2013-02-13\",4,0.76\n",
      "\"2013-02-14\",4,0\n",
      "\"2013-02-15\",8,0\n",
      "\"2013-02-16\",2,0.51\n",
      "\"2013-02-17\",-4,0\n",
      "\"2013-02-18\",-3,0\n",
      "\"2013-02-19\",5,3.81\n",
      "\"2013-02-20\",0,0\n",
      "\"2013-02-21\",-2,0\n",
      "\"2013-02-22\",0,0\n",
      "\"2013-02-23\",4,6.6\n",
      "\"2013-02-24\",5,0.25\n",
      "\"2013-02-25\",4,0\n",
      "\"2013-02-26\",4,3.56\n",
      "\"2013-02-27\",6,39.62\n",
      "\"2013-02-28\",7,0\n",
      "\"2013-03-01\",5,0\n",
      "\"2013-03-02\",2,0\n",
      "\"2013-03-03\",2,0\n",
      "\"2013-03-04\",2,0\n",
      "\"2013-03-05\",4,0\n",
      "\"2013-03-06\",4,0\n",
      "\"2013-03-07\",2,4.83\n",
      "\"2013-03-08\",3,14.22\n",
      "\"2013-03-09\",7,0\n",
      "\"2013-03-10\",6,0\n",
      "\"2013-03-11\",8,0\n",
      "\"2013-03-12\",10,20.07\n",
      "\"2013-03-13\",7,0\n",
      "\"2013-03-14\",2,0\n",
      "\"2013-03-15\",4,0\n",
      "\"2013-03-16\",3,3.05\n",
      "\"2013-03-17\",1,0\n",
      "\"2013-03-18\",0,15.24\n",
      "\"2013-03-19\",3,9.14\n",
      "\"2013-03-20\",4,0\n",
      "\"2013-03-21\",2,0\n",
      "\"2013-03-22\",2,0\n",
      "\"2013-03-23\",4,0\n",
      "\"2013-03-24\",4,0\n",
      "\"2013-03-25\",3,4.32\n",
      "\"2013-03-26\",7,0\n",
      "\"2013-03-27\",7,0\n",
      "\"2013-03-28\",7,0\n",
      "\"2013-03-29\",9,0\n",
      "\"2013-03-30\",10,0\n",
      "\"2013-03-31\",9,2.03\n",
      "\"2013-04-01\",10,0\n",
      "\"2013-04-02\",3,0\n",
      "\"2013-04-03\",4,0\n",
      "\"2013-04-04\",6,0\n",
      "\"2013-04-05\",12,0\n",
      "\"2013-04-06\",7,0\n",
      "\"2013-04-07\",9,0\n",
      "\"2013-04-08\",17,0\n",
      "\"2013-04-09\",19,0\n",
      "\"2013-04-10\",18,12.45\n",
      "\"2013-04-11\",12,0\n",
      "\"2013-04-12\",7,16\n",
      "\"2013-04-13\",10,0.25\n",
      "\"2013-04-14\",11,0\n",
      "\"2013-04-15\",11,0\n",
      "\"2013-04-16\",13,0\n",
      "\"2013-04-17\",17,0.51\n",
      "\"2013-04-18\",13,0.25\n",
      "\"2013-04-19\",17,1.27\n",
      "\"2013-04-20\",11,1.52\n",
      "\"2013-04-21\",8,0\n",
      "\"2013-04-22\",9,0\n",
      "\"2013-04-23\",8,0\n",
      "\"2013-04-24\",14,0\n",
      "\"2013-04-25\",13,0\n",
      "\"2013-04-26\",15,0\n",
      "\"2013-04-27\",16,0\n",
      "\"2013-04-28\",16,0\n",
      "\"2013-04-29\",13,1.02\n",
      "\"2013-04-30\",16,0\n",
      "\"2013-05-01\",14,0\n",
      "\"2013-05-02\",16,0\n",
      "\"2013-05-03\",14,0\n",
      "\"2013-05-04\",15,0\n",
      "\"2013-05-05\",13,0\n",
      "\"2013-05-06\",14,0\n",
      "\"2013-05-07\",17,0\n",
      "\"2013-05-08\",15,76.71\n",
      "\"2013-05-09\",16,12.7\n",
      "\"2013-05-10\",21,0.25\n",
      "\"2013-05-11\",19,27.69\n",
      "\"2013-05-12\",16,0\n",
      "\"2013-05-13\",11,0\n",
      "\"2013-05-14\",11,0\n",
      "\"2013-05-15\",17,0\n",
      "\"2013-05-16\",22,0\n",
      "\"2013-05-17\",18,0\n",
      "\"2013-05-18\",16,0.25\n",
      "\"2013-05-19\",14,15.24\n",
      "\"2013-05-20\",21,0\n",
      "\"2013-05-21\",25,0\n",
      "\"2013-05-22\",21,0\n",
      "\"2013-05-23\",22,45.97\n",
      "\"2013-05-24\",13,7.62\n",
      "\"2013-05-25\",10,3.56\n",
      "\"2013-05-26\",14,0\n",
      "\"2013-05-27\",17,0\n",
      "\"2013-05-28\",17,13.21\n",
      "\"2013-05-29\",21,0\n",
      "\"2013-05-30\",27,0\n",
      "\"2013-05-31\",28,0\n",
      "\"2013-06-01\",28,0\n",
      "\"2013-06-02\",26,21.59\n",
      "\"2013-06-03\",22,22.1\n",
      "\"2013-06-04\",19,0\n",
      "\"2013-06-05\",19,0\n",
      "\"2013-06-06\",18,3.3\n",
      "\"2013-06-07\",16,105.66\n",
      "\"2013-06-08\",19,12.19\n",
      "\"2013-06-09\",22,0\n",
      "\"2013-06-10\",19,35.05\n",
      "\"2013-06-11\",22,2.29\n",
      "\"2013-06-12\",22,0\n",
      "\"2013-06-13\",17,32\n",
      "\"2013-06-14\",17,9.65\n",
      "\"2013-06-15\",22,0\n",
      "\"2013-06-16\",23,0\n",
      "\"2013-06-17\",25,0.25\n",
      "\"2013-06-18\",23,4.83\n",
      "\"2013-06-19\",20,0.25\n",
      "\"2013-06-20\",22,0\n",
      "\"2013-06-21\",23,0\n",
      "\"2013-06-22\",24,0\n",
      "\"2013-06-23\",26,0\n",
      "\"2013-06-24\",28,0\n",
      "\"2013-06-25\",28,0\n",
      "\"2013-06-26\",27,1.27\n",
      "\"2013-06-27\",27,6.1\n",
      "\"2013-06-28\",26,0\n",
      "\"2013-06-29\",25,0\n",
      "\"2013-06-30\",27,0\n",
      "\"2013-07-01\",24,21.34\n",
      "\"2013-07-02\",25,2.03\n",
      "\"2013-07-03\",26,13.46\n",
      "\"2013-07-04\",27,0\n",
      "\"2013-07-05\",28,0\n",
      "\"2013-07-06\",29,0\n",
      "\"2013-07-07\",29,0\n",
      "\"2013-07-08\",27,5.59\n",
      "\"2013-07-09\",27,5.84\n",
      "\"2013-07-10\",27,0\n",
      "\"2013-07-11\",27,0\n",
      "\"2013-07-12\",23,6.35\n",
      "\"2013-07-13\",23,1.52\n",
      "\"2013-07-14\",28,0\n",
      "\"2013-07-15\",30,0\n",
      "\"2013-07-16\",30,0\n",
      "\"2013-07-17\",31,0\n",
      "\"2013-07-18\",32,0\n",
      "\"2013-07-19\",32,0\n",
      "\"2013-07-20\",31,0\n",
      "\"2013-07-21\",28,0\n",
      "\"2013-07-22\",27,1.52\n",
      "\"2013-07-23\",27,7.87\n",
      "\"2013-07-24\",24,0\n",
      "\"2013-07-25\",19,0.25\n",
      "\"2013-07-26\",23,0\n",
      "\"2013-07-27\",24,0\n",
      "\"2013-07-28\",23,6.1\n",
      "\"2013-07-29\",25,0.25\n",
      "\"2013-07-30\",24,0\n",
      "\"2013-07-31\",24,0\n",
      "\"2013-08-01\",22,16.51\n",
      "\"2013-08-02\",24,0\n",
      "\"2013-08-03\",23,1.52\n",
      "\"2013-08-04\",23,0\n",
      "\"2013-08-05\",21,0\n",
      "\"2013-08-06\",23,0\n",
      "\"2013-08-07\",24,0\n",
      "\"2013-08-08\",24,11.68\n",
      "\"2013-08-09\",27,1.27\n",
      "\"2013-08-10\",25,0\n",
      "\"2013-08-11\",23,0\n",
      "\"2013-08-12\",24,1.27\n",
      "\"2013-08-13\",23,21.59\n",
      "\"2013-08-14\",20,0\n",
      "\"2013-08-15\",21,0\n",
      "\"2013-08-16\",23,0\n",
      "\"2013-08-17\",23,0\n",
      "\"2013-08-18\",22,0\n",
      "\"2013-08-19\",23,0\n",
      "\"2013-08-20\",26,0\n",
      "\"2013-08-21\",27,0\n",
      "\"2013-08-22\",24,6.35\n",
      "\"2013-08-23\",25,0\n",
      "\"2013-08-24\",23,0\n",
      "\"2013-08-25\",23,0\n",
      "\"2013-08-26\",24,1.02\n",
      "\"2013-08-27\",26,0.25\n",
      "\"2013-08-28\",26,10.92\n",
      "\"2013-08-29\",24,0\n",
      "\"2013-08-30\",26,0\n",
      "\"2013-08-31\",27,0\n",
      "\"2013-09-01\",27,0\n",
      "\"2013-09-02\",26,1.27\n",
      "\"2013-09-03\",24,0.76\n",
      "\"2013-09-04\",23,0\n",
      "\"2013-09-05\",22,0\n",
      "\"2013-09-06\",18,0\n",
      "\"2013-09-07\",21,0\n",
      "\"2013-09-08\",23,0\n",
      "\"2013-09-09\",18,0\n",
      "\"2013-09-10\",26,0.25\n",
      "\"2013-09-11\",31,0\n",
      "\"2013-09-12\",26,40.64\n",
      "\"2013-09-13\",20,1.52\n",
      "\"2013-09-14\",16,0\n",
      "\"2013-09-15\",17,0\n",
      "\"2013-09-16\",18,0.76\n",
      "\"2013-09-17\",14,0\n",
      "\"2013-09-18\",17,0\n",
      "\"2013-09-19\",19,0\n",
      "\"2013-09-20\",21,0\n",
      "\"2013-09-21\",21,18.29\n",
      "\"2013-09-22\",17,11.43\n",
      "\"2013-09-23\",14,0\n",
      "\"2013-09-24\",16,0\n",
      "\"2013-09-25\",17,0\n",
      "\"2013-09-26\",18,0\n",
      "\"2013-09-27\",17,0\n",
      "\"2013-09-28\",18,0\n",
      "\"2013-09-29\",18,0\n",
      "\"2013-09-30\",19,0\n",
      "\"2013-10-01\",22,0\n",
      "\"2013-10-02\",23,0\n",
      "\"2013-10-03\",22,0\n",
      "\"2013-10-04\",24,0\n",
      "\"2013-10-05\",21,0\n",
      "\"2013-10-06\",20,0\n",
      "\"2013-10-07\",20,6.35\n",
      "\"2013-10-08\",16,0\n",
      "\"2013-10-09\",14,0\n",
      "\"2013-10-10\",16,0\n",
      "\"2013-10-11\",18,0.51\n",
      "\"2013-10-12\",19,0\n",
      "\"2013-10-13\",16,0\n",
      "\"2013-10-14\",15,0\n",
      "\"2013-10-15\",17,0\n",
      "\"2013-10-16\",17,0\n",
      "\"2013-10-17\",19,0.51\n",
      "\"2013-10-18\",17,0\n",
      "\"2013-10-19\",14,0.25\n",
      "\"2013-10-20\",14,0\n",
      "\"2013-10-21\",14,0\n",
      "\"2013-10-22\",15,0\n",
      "\"2013-10-23\",10,0\n",
      "\"2013-10-24\",9,0\n",
      "\"2013-10-25\",8,0\n",
      "\"2013-10-26\",9,0\n",
      "\"2013-10-27\",11,0\n",
      "\"2013-10-28\",11,0\n",
      "\"2013-10-29\",10,0\n",
      "\"2013-10-30\",12,0\n",
      "\"2013-10-31\",16,1.52\n",
      "\"2013-11-01\",18,3.3\n",
      "\"2013-11-02\",17,0\n",
      "\"2013-11-03\",8,0\n",
      "\"2013-11-04\",5,0\n",
      "\"2013-11-05\",9,0\n",
      "\"2013-11-06\",13,0\n",
      "\"2013-11-07\",12,3.3\n",
      "\"2013-11-08\",7,0\n",
      "\"2013-11-09\",7,0\n",
      "\"2013-11-10\",12,0\n",
      "\"2013-11-11\",9,0\n",
      "\"2013-11-12\",6,0.76\n",
      "\"2013-11-13\",1,0\n",
      "\"2013-11-14\",6,0\n",
      "\"2013-11-15\",11,0\n",
      "\"2013-11-16\",12,1.27\n",
      "\"2013-11-17\",13,0.76\n",
      "\"2013-11-18\",14,5.59\n",
      "\"2013-11-19\",7,0\n",
      "\"2013-11-20\",3,0\n",
      "\"2013-11-21\",7,0\n",
      "\"2013-11-22\",12,1.78\n",
      "\"2013-11-23\",6,0\n",
      "\"2013-11-24\",-3,0\n",
      "\"2013-11-25\",-2,0\n",
      "\"2013-11-26\",4,12.95\n",
      "\"2013-11-27\",9,50.29\n",
      "\"2013-11-28\",1,0\n",
      "\"2013-11-29\",1,0\n",
      "\"2013-11-30\",0,0\n",
      "\"2013-12-01\",6,0\n",
      "\"2013-12-02\",7,0\n",
      "\"2013-12-03\",8,0\n",
      "\"2013-12-04\",8,0\n",
      "\"2013-12-05\",12,0.25\n",
      "\"2013-12-06\",10,18.54\n",
      "\"2013-12-07\",3,3.56\n",
      "\"2013-12-08\",-1,2.03\n",
      "\"2013-12-09\",2,7.62\n",
      "\"2013-12-10\",1,5.84\n",
      "\"2013-12-11\",-1,0\n",
      "\"2013-12-12\",-3,0\n",
      "\"2013-12-13\",-2,0\n",
      "\"2013-12-14\",-2,18.54\n",
      "\"2013-12-15\",2,18.29\n",
      "\"2013-12-16\",-2,0\n",
      "\"2013-12-17\",-2,4.83\n",
      "\"2013-12-18\",-1,0\n",
      "\"2013-12-19\",4,0\n",
      "\"2013-12-20\",8,0\n",
      "\"2013-12-21\",14,0.25\n",
      "\"2013-12-22\",19,0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "\"\"2013-01-01\",1,0\n",
       "\"2013-01-02\",-2,0\n",
       "\"2013-01-03\",-2,0\n",
       "\"2013-01-04\",1,0\n",
       "\"2013-01-05\",3,0\n",
       "\"2013-01-06\",4,0\n",
       "\"2013-01-07\",5,0\n",
       "\"2013-01-08\",6,0\n",
       "\"2013-01-09\",7,0\n",
       "\"2013-01-10\",7,0\n",
       "\"2013-01-11\",6,13.97\n",
       "\"2013-01-12\",7,0.51\n",
       "\"2013-01-13\",8,0\n",
       "\"2013-01-14\",8,2.29\n",
       "\"2013-01-15\",3,3.05\n",
       "\"2013-01-16\",2,17.53\n",
       "\"2013-01-17\",4,0\n",
       "\"2013-01-18\",-1,0\n",
       "\"2013-01-19\",5,0\n",
       "\"2013-01-20\",6,0\n",
       "\"2013-01-21\",-2,0\n",
       "\"2013-01-22\",-7,0\n",
       "\"2013-01-23\",-9,0\n",
       "\"2013-01-24\",-8,0\n",
       "\"2013-01-25\",-7,1.78\n",
       "\"2013-01-26\",-6,0\n",
       "\"2013-01-27\",-3,0\n",
       "\"2013-01-28\",1,5.59\n",
       "\"2013-01-29\",6,1.52\n",
       "\"2013-01-30\",9,1.02\n",
       "\"2013-01-31\",8,22.86\n",
       "\"2013-02-01\",-2,0\n",
       "\"2013-02-02\",-4,0.51\n",
       "\"2013-02-03\",-3,0.51\n",
       "\"2013-02-04\",-3,0\n",
       "\"2013-02-05\",-1,0.51\n",
       "\"2013-02-06\",1,0\n",
       "\"2013-02-07\",-2,0\n",
       "\"2013-02-08\",-1,29.21\n",
       "\"2013-02-09\",-3,9.65\n",
       "\"2013-02-10\",-3,0\n",
       "\"2013-02-11\",4,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "val lines = scala.io.Source.fromFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nycweather.csv\").mkString\n",
    "println(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are three columns in the dataset, the date, the mean temperature in Celsius, and the precipitation for the day. Since we already know the schema, we will infer the schema using reflection.\n",
    "\n",
    "You will first need to define the SparkSQL context. Do so by creating it from an existing SparkContext. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@2aeb52b6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@2aeb52b6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, you need to import a library for creating a SchemaRDD. Type this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a case class in Scala that defines the schema of the table. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Weather\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "case class Weather(date: String, temp: Int, precipitation: Double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the RDD of the Weather object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather = [date: string, temp: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, temp: int ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nycweather.csv\").map(_.split(\",\")). map(w => Weather(w(0), w(1).trim.toInt, w(2).trim.toDouble)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You first load in the file, and then you map it by splitting it up by the commas and then another mapping to get it into the Weather class.\n",
    "\n",
    "Next you need to register the RDD as a table. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "weather.registerTempTable(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At this point, you are ready to create and run some queries on the RDD. You want to get a list of the hottest dates with some precipitation. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hottest_with_precip = [date: string, temp: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array([\"2013-06-26\",27,1.27], [\"2013-06-27\",27,6.1], [\"2013-07-08\",27,5.59], [\"2013-07-09\",27,5.84], [\"2013-07-22\",27,1.52], [\"2013-07-23\",27,7.87], [\"2013-08-09\",27,1.27], [\"2013-06-02\",26,21.59], [\"2013-07-03\",26,13.46], [\"2013-08-27\",26,0.25], [\"2013-08-28\",26,10.92], [\"2013-09-02\",26,1.27], [\"2013-09-10\",26,0.25], [\"2013-09-12\",26,40.64], [\"2013-06-17\",25,0.25], [\"2013-07-02\",25,2.03], [\"2013-07-29\",25,0.25], [\"2013-07-01\",24,21.34], [\"2013-08-08\",24,11.68], [\"2013-08-12\",24,1.27], [\"2013-08-22\",24,6.35], [\"2013-08-26\",24,1.02], [\"2013-09-03\",24,0.76], [\"2013-06-18\",23,4.83], [\"2013-07-12\",23,6.35], [\"2013-07-13\",23,1.52], [\"2013-07-28\",23,6.1], [..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hottest_with_precip = sqlContext.sql(\"SELECT * FROM weather WHERE precipitation > 0.0 ORDER BY temp DESC\")\n",
    "\n",
    "hottest_with_precip.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Normal RDD operations will work. Print the top hottest days with some precipitation out to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:38: error: value top is not a member of org.apache.spark.sql.Dataset[(String, String, String)]\n",
       "       hottest_with_precip.map(x => (\"Date: \" + x(0), \"Temp : \" + x(1), \"Precip: \" + x(2))).top(10).foreach(println)\n",
       "                                                                                            ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_with_precip.map(x => (\"Date: \" + x(0), \"Temp : \" + x(1), \"Precip: \" + x(2))).top(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Spark application using MLlib\n",
    "\n",
    "In this section, Spark will be used to acquire the K-Means clustering for drop-off latitudes and longitudes of taxis for 3 clusters. The sample data contains a subset of taxi trips with hack license, medallion, pickup date/time, drop off date/time, pickup/drop off latitude/longitude, passenger count, trip distance, trip time and other information. As such, this may give a good indication of where to best to hail a cab.\n",
    "\n",
    "Remember, this is only a subset of the file that you used in a previous exercise. If you ran this exercise on the full dataset, it would take a long time as we are only running on a test environment with limited resources.\n",
    "\n",
    "Import the needed packages for K-Means algorithm and Vector packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiFile = /resources/jupyterlab/labs/BD0211EN/LabData/nyctaxisub.csv MapPartitionsRDD[416] at textFile at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxisub.csv MapPartitionsRDD[416] at textFile at <console>:50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiFile = sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/nyctaxisub.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Determine the number of rows in taxiFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Cleanse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiData = MapPartitionsRDD[17] at filter at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at filter at <console>:41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiData=taxiFile.filter(_.contains(\"2013\")).\n",
    "    filter(_.split(\",\")(3)!=\"\" ).    //dropoff_latitude\n",
    "    filter(_.split(\",\")(4)!=\"\")      //dropoff_longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first filter limits the rows to those that occurred in the year 2013. This will also remove any header in the file. The third and fourth columns contain the drop off latitude and longitude. The transformation will throw exceptions if these values are empty.\n",
    "\n",
    "Do another count to see what was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this case, if we had used the full set of data, it would have filtered out a great many more lines.\n",
    "\n",
    "To fence the area roughly to New York City use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiFence = MapPartitionsRDD[21] at filter at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[21] at filter at <console>:45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiFence=taxiData.\n",
    "    filter(_.split(\",\")(3).toDouble>40.70).\n",
    "    filter(_.split(\",\")(3).toDouble<40.86).\n",
    "    filter(_.split(\",\")(4).toDouble>(-74.02)).\n",
    "    filter(_.split(\",\")(4).toDouble<(-73.93))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Determine how many are left in taxiFence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206646"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiFence.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Approximately, 43,354 rows were dropped since these drop-off points are outside of New York City.\n",
    "\n",
    "Create Vectors with the latitudes and longitudes that will be used as input to the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi = MapPartitionsRDD[22] at map at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[22] at map at <console>:44"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxi=taxiFence.\n",
    "    map{\n",
    "        line=>Vectors.dense(\n",
    "            line.split(',').slice(3,5).map(_ .toDouble)\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40.756797405990476,-73.98101633586084)\n",
      "(40.78693111696048,-73.95721159368627)\n",
      "(40.72455584513588,-73.99592835075228)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iterationCount = 10\n",
       "clusterCount = 3\n",
       "model = org.apache.spark.mllib.clustering.KMeansModel@7e1ac2bf\n",
       "clusterCenters = Array(Array(40.756797405990476, -73.98101633586084), Array(40.78693111696048, -73.95721159368627), Array(40.72455584513588, -73.99592835075228))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(Array(40.756797405990476, -73.98101633586084), Array(40.78693111696048, -73.95721159368627), Array(40.72455584513588, -73.99592835075228))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iterationCount=10\n",
    "val clusterCount=3\n",
    "\n",
    "val model=KMeans.train(taxi,clusterCount,iterationCount)\n",
    "val clusterCenters=model.clusterCenters.map(_.toArray)\n",
    "\n",
    "clusterCenters.foreach(lines=>println(lines(0),lines(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we know the map co-ordinates. Not surprisingly, the second point is between the Theater District and Grand Central. The third point is in The Village, NYU, Soho and Little Italy area. The first point is the Upper East Side, presumably where people are more likely to take cabs than subways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Spark application using Spark Streaming\n",
    "\n",
    "This section focuses on Spark Streams, an easy to build, scalable, stateful (e.g. sliding windows) stream processing library. Streaming jobs are written the same way Spark batch jobs are coded and support Java, Scala and Python. In this exercise, taxi trip data will be streamed using a socket connection and then analyzed to provide a summary of number of passengers by taxi vendor. This will be implemented in the Spark shell using Scala.\n",
    "\n",
    "There are two relevant files for this section. The first one is the nyctaxi100.csv which will serve as the source of the stream. The other file is a python file, taxistreams.py, which will feed the csv file through a socket connection to simulate a stream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the StreamingContext by using the existing SparkContext (sc). It will be using a 1 second batch interval, which means the stream is divided to 1 second batches and each batch becomes a RDD. This is intentional to make it easier to read the data during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssc = org.apache.spark.streaming.StreamingContext@13dea920\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.StreamingContext@13dea920"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the socket stream that connects to the localhost socket 7888. This matches the port that the Python script is listening on. Each batch from the Stream be a lines RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines = org.apache.spark.streaming.dstream.SocketInputDStream@2498ba30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.dstream.SocketInputDStream@2498ba30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = ssc.socketTextStream(\"localhost\", 7888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, put in the business logic to split up the lines on each comma and mapping pass(15), which is the vendor, and pass(7), which is the passenger count. Then this is reduced by key resulting in a summary of number of passengers by vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pass = org.apache.spark.streaming.dstream.ShuffledDStream@2b3d8413\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.dstream.ShuffledDStream@2b3d8413"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pass = lines.map(_.split(\",\")).\n",
    "    map(pass=>(pass(15), pass(7).toInt)).\n",
    "    reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print out to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pass.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next two line starts the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1556992706000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992707000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992708000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992709000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992710000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",5)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992711000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992712000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992713000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992714000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992715000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992716000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992717000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992718000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992719000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992720000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992721000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992722000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992723000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992724000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992725000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992726000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992727000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992728000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992729000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992730000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992731000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992732000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992733000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",5)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992734000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992735000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992736000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992737000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992738000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992739000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992740000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992741000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992742000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992743000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992744000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",7)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992745000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992746000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992747000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992748000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992749000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992750000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992751000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992752000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992753000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",8)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992754000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "(\"VTS\",2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.InterruptedException\n",
       "Message: null\n",
       "StackTrace:   at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)\n",
       "  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)\n",
       "  at org.apache.spark.streaming.ContextWaiter.waitForStopOrError(ContextWaiter.scala:63)\n",
       "  at org.apache.spark.streaming.StreamingContext.awaitTermination(StreamingContext.scala:618)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1556992755000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992756000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992757000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992758000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992759000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992760000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992761000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992762000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992763000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992764000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992765000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992766000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992767000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992768000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992769000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992770000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992771000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992772000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992773000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992774000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992775000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992776000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992777000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992778000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992779000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992780000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992781000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992782000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992783000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992784000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1556992785000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It will take a few cycles for the connection to be recognized, and then the data is sent. In this case, 2 rows per second of taxi trip data is receive in a 1 second batch interval.\n",
    "\n",
    "In the Python terminal, the contents of the file are printed as they are streamed.\n",
    "\n",
    "This is just a simple example showing how you can take streaming data into Spark and do some type of processing on it. In the case here, the taxi and the number of passengers was extracted from the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a Spark application using GraphX\n",
    "\n",
    "Users.txt is a set of users and followers is the relationship between the users. Take a look at the contents of these two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: \n",
      "1,BarackObama,Barack Obama\n",
      "2,ladygaga,Goddess of Love\n",
      "3,jeresig,John Resig\n",
      "4,justinbieber,Justin Bieber\n",
      "6,matei_zaharia,Matei Zaharia\n",
      "7,odersky,Martin Odersky\n",
      "8,anonsys\n",
      "\n",
      "Followers: \n",
      "2 1\n",
      "4 1\n",
      "1 2\n",
      "6 3\n",
      "7 3\n",
      "7 6\n",
      "6 7\n",
      "3 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Users: \")\n",
    "println(scala.io.Source.fromFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/users.txt\").mkString)\n",
    "\n",
    "println(\"Followers: \")\n",
    "println(scala.io.Source.fromFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/followers.txt\").mkString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import the GraphX package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the users RDD and parse into tuples of user id and attribute list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,[Ljava.lang.String;@7288e7f)\n",
      "(2,[Ljava.lang.String;@1b556797)\n",
      "(3,[Ljava.lang.String;@25b42ee8)\n",
      "(4,[Ljava.lang.String;@662c1757)\n",
      "(6,[Ljava.lang.String;@470bfe30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users = MapPartitionsRDD[3] at map at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at map at <console>:30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = (sc.textFile(\"/resources/jupyterlab/labs/BD0211EN/LabData/users.txt\").map(line => line.split(\",\")).map(parts => (parts.head.toLong, parts.tail)))\n",
    "\n",
    "users.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Parse the edge data, which is already in userId -> userId format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "followerGraph = org.apache.spark.graphx.impl.GraphImpl@10f925d3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@10f925d3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val followerGraph = GraphLoader.edgeListFile(sc, \"/resources/jupyterlab/labs/BD0211EN/LabData/followers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Attach the user attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph = org.apache.spark.graphx.impl.GraphImpl@7b18b2e6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@7b18b2e6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = followerGraph.outerJoinVertices(users) {\n",
    "    case (uid, deg, Some(attrList)) => attrList\n",
    "    case (uid, deg, None) => Array.empty[String]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Restrict the graph to users with usernames and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subgraph = org.apache.spark.graphx.impl.GraphImpl@209bdb8c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@209bdb8c"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute the PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pagerankGraph = org.apache.spark.graphx.impl.GraphImpl@1cf87911\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@1cf87911"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pagerankGraph = subgraph.pageRank(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the attributes of the top pagerank users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userInfoWithPageRank = org.apache.spark.graphx.impl.GraphImpl@1948d61b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@1948d61b"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {\n",
    "    case (uid, attrList, Some(pr)) => (pr, attrList.toList)\n",
    "    case (uid, attrList, None) => (0.0, attrList.toList)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print the line out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(1.4610558475474507,List(BarackObama, Barack Obama)))\n",
      "(2,(1.3926425103962674,List(ladygaga, Goddess of Love)))\n",
      "(7,(1.2956193310217194,List(odersky, Martin Odersky)))\n",
      "(3,(0.9985540153884633,List(jeresig, John Resig)))\n",
      "(6,(0.7013832556651652,List(matei_zaharia, Matei Zaharia)))\n"
     ]
    }
   ],
   "source": [
    "println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
